<script>
  const video = document.getElementById('webcam');
  const canvas = document.getElementById('canvas');
  const ctx = canvas.getContext('2d');
  const overlayBox = document.getElementById('overlayBox');
  const switchBtn = document.getElementById('switchBtn');
  const languageBtn = document.getElementById('languageBtn');
  const micBtn = document.getElementById('micBtn');

  let stream;
  let apiKey = '';
  let usingFrontCamera = false;
  let analyzing = false;
  let currentLanguage = 'en';
  let recognition;

  fetch("api.txt")
    .then(res => res.text())
    .then(key => apiKey = key.trim());

  async function setupWebcam() {
    if (stream) {
      stream.getTracks().forEach(track => track.stop());
    }

    const constraints = {
      video: {
        facingMode: usingFrontCamera ? "user" : "environment"
      },
      audio: false
    };

    stream = await navigator.mediaDevices.getUserMedia(constraints);
    video.srcObject = stream;

    return new Promise(resolve => {
      video.onloadedmetadata = () => resolve();
    });
  }

  function speak(text, lang = 'en') {
    const synth = window.speechSynthesis;
    synth.cancel();
    const utterance = new SpeechSynthesisUtterance(text);
    utterance.lang = lang === 'hi' ? 'hi-IN' : 'en-US';
    synth.speak(utterance);
  }

  async function captureFrame() {
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    return new Promise((resolve) => {
      canvas.toBlob(blob => {
        const reader = new FileReader();
        reader.onloadend = () => resolve(reader.result.split(',')[1]);
        reader.readAsDataURL(blob);
      }, 'image/jpeg');
    });
  }

  async function scanWithGemini() {
    if (analyzing) return;
    analyzing = true;
    overlayBox.innerText = currentLanguage === 'hi' ? "à¤µà¤¿à¤¶à¥à¤²à¥‡à¤·à¤£ à¤•à¤¿à¤¯à¤¾ à¤œà¤¾ à¤°à¤¹à¤¾ à¤¹à¥ˆ..." : "Analyzing...";

    const base64Image = await captureFrame();

    try {
      const response = await fetch(
        `https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent?key=${apiKey}`,
        {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            contents: [{
              parts: [
                { inlineData: { mimeType: 'image/jpeg', data: base64Image } },
                { text: "What do you see in this image in simple terms?" }
              ]
            }]
          })
        }
      );

      const data = await response.json();
      const message = data?.candidates?.[0]?.content?.parts?.[0]?.text || (currentLanguage === 'hi' ? "à¤›à¤µà¤¿ à¤•à¤¾ à¤µà¤¿à¤¶à¥à¤²à¥‡à¤·à¤£ à¤¨à¤¹à¥€à¤‚ à¤•à¤¿à¤¯à¤¾ à¤œà¤¾ à¤¸à¤•à¤¾à¥¤" : "Could not analyze image.");

      overlayBox.innerText = message;
      speak(message, currentLanguage);
    } catch (err) {
      overlayBox.innerText = currentLanguage === 'hi' ? "à¤µà¤¿à¤¶à¥à¤²à¥‡à¤·à¤£ à¤µà¤¿à¤«à¤² à¤°à¤¹à¤¾à¥¤" : "Failed to analyze.";
      console.error(err);
    } finally {
      analyzing = false;
    }
  }

  function startVoiceCommand() {
    if (!('webkitSpeechRecognition' in window || 'SpeechRecognition' in window)) {
      overlayBox.innerText = "ðŸŽ™ï¸ Your browser does not support voice recognition.";
      return;
    }

    if (recognition) recognition.stop();

    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    recognition = new SpeechRecognition();
    recognition.lang = currentLanguage === 'hi' ? 'hi-IN' : 'en-US';
    recognition.interimResults = false;
    recognition.continuous = true;

    recognition.onresult = (event) => {
      const transcript = event.results[event.results.length - 1][0].transcript.trim().toLowerCase();
      console.log("Heard:", transcript);
      const keywords = [
        "what is this",
        "ye kya hai",
        "à¤¯à¤¹ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ",
        "à¤µà¥à¤¹à¤¾à¤Ÿ à¤‡à¤¸ à¤¦à¤¿à¤¸"
      ];
      if (keywords.some(keyword => transcript.includes(keyword))) {
        scanWithGemini();
      }
    };

    recognition.onerror = (e) => {
      console.warn("Speech error:", e.error);
      setTimeout(() => startVoiceCommand(), 1000);
    };

    recognition.onend = () => {
      setTimeout(() => startVoiceCommand(), 1000);
    };

    recognition.start();
    overlayBox.innerText = currentLanguage === 'hi'
      ? "ðŸŽ¤ à¤®à¤¾à¤‡à¤•à¥à¤°à¥‹à¤«à¥‹à¤¨ à¤šà¤¾à¤²à¥‚ à¤¹à¥ˆ..."
      : "ðŸŽ¤ Microphone is active...";
  }

  // Button Listeners
  switchBtn.addEventListener('click', () => {
    usingFrontCamera = !usingFrontCamera;
    setupWebcam();
  });

  languageBtn.addEventListener('click', () => {
    currentLanguage = currentLanguage === 'en' ? 'hi' : 'en';
    languageBtn.innerText = currentLanguage === 'en' ? 'Convert to Hindi' : 'Convert to English';
    overlayBox.innerText = currentLanguage === 'hi' ? "JARVIS à¤‘à¤¨à¤²à¤¾à¤‡à¤¨ à¤¹à¥ˆ..." : "JARVIS is online...";
  });

  micBtn.addEventListener('click', () => {
    startVoiceCommand();
  });

  // Startup
  setupWebcam().then(() => {
    requestAnimationFrame(updateVideoCanvas);
  });

  function updateVideoCanvas() {
    if (video.readyState >= 2) {
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    }
    requestAnimationFrame(updateVideoCanvas);
  }
</script>
